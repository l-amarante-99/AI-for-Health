{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Health\n",
    "## Detecting Active Tuberculosis Bacilli on TB Smears\n",
    "### Group: Luciana, Seohee and Irma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBWG_ROOT = Path().home() / \"datasets\" / \"tb-wellgen-smear\"\n",
    "CONTEST_DIR = TBWG_ROOT / \"supplementary\"\n",
    "IMAGE_ROOT = TBWG_ROOT / \"images\"\n",
    "TABLE_DIR = TBWG_ROOT / \"v1\"\n",
    "LOGGER_DIR = Path().home() / \"project\" / \"logs\"\n",
    "\n",
    "tb_labels_df = pd.read_csv(TABLE_DIR / \"tb-labels.csv\")\n",
    "weights_path = os.path.join(\"Pretrained_Weights\", \"resnet18-f37072fd.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Training and Test Sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, val_size):\n",
    "    \"\"\"Helper function to split training and test sets\"\"\"\n",
    "    n = len(df)\n",
    "    idxs = np.random.randint(0, n, size=val_size)\n",
    "    test_df = df.iloc[idxs].copy().reset_index(drop=True)\n",
    "    train_df = df.drop(index=idxs).reset_index(drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = random_split(tb_labels_df, 1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function to Fetch Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(idx, df, numpy=False, to_float=False):\n",
    "    \"\"\"Helper function to fetch image from disk\"\"\"\n",
    "    path = df.loc[idx, \"file_path\"]\n",
    "    assert os.path.exists(path)\n",
    "    img = Image.open(path)\n",
    "    if numpy:\n",
    "        arr = np.asarray(img)\n",
    "        if to_float:\n",
    "            arr = arr / 255.0\n",
    "        return arr\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "img = get_image(5, train_df)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Means and Standard Deviations for Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "\n",
    "for idx in range(1000):\n",
    "    arr = get_image(idx, train_df, numpy=True, to_float=True)\n",
    "    means.append(arr.mean(axis=(0, 1)))\n",
    "    stds.append(arr.std(axis=(0, 1)))\n",
    "\n",
    "MEANS = np.vstack(means).mean(axis=0)\n",
    "STDS = np.vstack(stds).mean(axis=0)\n",
    "\n",
    "print(MEANS, STDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tbDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, train=True):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            self.train = self.df.sample(frac=1.0, ignore_index=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = get_image(idx, self.df, numpy=True, to_float=True)\n",
    "        label = int(self.df.loc[idx, \"tb_positive\"])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataLoaders in Lightning DataModule Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tbDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, batch_size=64, num_workers=8):\n",
    "        super().__init__()\n",
    "        self.full_df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((224, 224), antialias=True), # input size for ResNet \n",
    "                transforms.Normalize(MEANS, STDS),\n",
    "            ]\n",
    "        )\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"train\" or stage == None:\n",
    "            self.train_df, self.test_df = random_split(self.full_df, 1000)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            tbDataset(self.train_df, transform=self.transform),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            tbDataset(self.test_df, transform=self.transform),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LightningModule for Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tbModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.net = resnet18(weights=None, num_classes=self.num_classes)\n",
    "        self.net.double()\n",
    "\n",
    "        pretrained_weights = torch.load(weights_path) # Load the pre-trained weights\n",
    "        self.net.load_state_dict(pretrained_weights, strict=False) # Load the weights into the model, ignoring mismatched layers\n",
    "        self.net.fc = nn.Linear(self.net.fc.in_features, self.num_classes) # Replace the final fully connected layer\n",
    "\n",
    "        self.test_accuracy = Accuracy(task=\"binary\", num_classes=self.num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net.forward(X)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "        logits = self.net.forward(imgs)\n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "        logits = self.net.forward(imgs)\n",
    "        loss = nn.functional.cross_entropy(logits, labels) \n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        self.test_accuracy.update(preds, labels)\n",
    "        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = tbDataModule(tb_labels_df)\n",
    "\n",
    "model = tbModel()\n",
    "model = model.to(device)\n",
    "\n",
    "logger = pl.loggers.CSVLogger(save_dir=LOGGER_DIR, name=\"tb_demo7\")\n",
    "\n",
    "# Using early stopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss', # loss calculated on the validation set\n",
    "    patience=3, # early stop if loss doesn't improve after 3 epochs\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs = 50, \n",
    "    logger=logger,\n",
    "    callbacks=[early_stop_callback] # early stopping\n",
    ")\n",
    "\n",
    "trainer.fit(model, data)\n",
    "trainer.test(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
